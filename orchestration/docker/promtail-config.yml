# Promtail Configuration for Investment Signals Orchestration
#
# Promtail is an agent which ships the contents of local logs to Loki.
# This configuration collects logs from:
# - Airflow containers (scheduler, webserver, worker)
# - Health check service
# - Application logs
#
# Features:
# - Structured log parsing (JSON)
# - Automatic label extraction
# - Pipeline stages for log transformation
#
# Usage:
#   docker-compose up -d promtail
#   Logs are sent to Loki at http://loki:3100

server:
  http_listen_port: 9080
  grpc_listen_port: 0

# Position file for tracking log file positions
positions:
  filename: /tmp/positions.yaml

# Loki client configuration
clients:
  - url: http://loki:3100/loki/api/v1/push
    tenant_id: investment-signals
    batchwait: 1s
    batchsize: 1048576
    timeout: 10s
    backoff_config:
      min_period: 500ms
      max_period: 5m
      max_retries: 10
    external_labels:
      environment: ${ORCHESTRATION_ENV:-dev}
      service: investment-signals

# Scrape configurations
scrape_configs:
  # Airflow logs from container stdout/stderr
  - job_name: airflow
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
        filters:
          - name: label
            values: ["com.docker.compose.service=airflow-webserver", "com.docker.compose.service=airflow-scheduler", "com.docker.compose.service=airflow-worker", "com.docker.compose.service=airflow-triggerer"]
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.+)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
        target_label: 'service'
      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']
        target_label: 'project'
    pipeline_stages:
      - docker: {}
      - multiline:
          firstline: '^\[\d{4}-\d{2}-\d{2}'
          max_wait_time: 3s
      # Parse Airflow structured logs
      - regex:
          expression: '^\[(?P<timestamp>\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}[,\.]\d+)\]\s+\{(?P<file>[^}]+)\}\s+(?P<level>\w+)\s+-\s+(?P<message>.*)$'
      - labels:
          level:
          file:
      - timestamp:
          source: timestamp
          format: '2006-01-02T15:04:05.000'
          fallback_formats:
            - '2006-01-02 15:04:05,000'
      - output:
          source: message

  # Health check service logs
  - job_name: healthcheck
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
        filters:
          - name: label
            values: ["com.docker.compose.service=healthcheck"]
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.+)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
        target_label: 'service'
    pipeline_stages:
      - docker: {}
      # Parse JSON logs from health check service
      - json:
          expressions:
            level: level
            message: message
            timestamp: timestamp
            pipeline: pipeline
            task: task
            duration: duration
            error: error
      - labels:
          level:
          pipeline:
          task:
      - timestamp:
          source: timestamp
          format: RFC3339Nano
      - output:
          source: message

  # DAG logs from file system
  - job_name: dag_logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: dag_logs
          __path__: /var/log/airflow/dags/**/*.log
    pipeline_stages:
      - multiline:
          firstline: '^\[\d{4}-\d{2}-\d{2}'
          max_wait_time: 3s
      # Extract DAG and task information from log path
      - regex:
          expression: '/var/log/airflow/dags/(?P<dag_id>[^/]+)/(?P<run_id>[^/]+)/(?P<task_id>[^/]+)/.*\.log$'
          source: filename
      - labels:
          dag_id:
          task_id:
      # Parse log line
      - regex:
          expression: '^\[(?P<timestamp>\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}[,\.]\d+)\]\s+\{(?P<file>[^}]+)\}\s+(?P<level>\w+)\s+-\s+(?P<message>.*)$'
      - labels:
          level:
      - timestamp:
          source: timestamp
          format: '2006-01-02T15:04:05.000'
      - output:
          source: message

  # Application logs (structured JSON)
  - job_name: application
    static_configs:
      - targets:
          - localhost
        labels:
          job: application
          __path__: /var/log/investment-signals/**/*.log
    pipeline_stages:
      # Parse JSON application logs
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            pipeline: pipeline
            task: task
            dag_id: dag_id
            run_id: run_id
            ticker: ticker
            signal_type: signal_type
            score: score
            api_provider: api_provider
            api_cost: api_cost
            duration_ms: duration_ms
            error: error
            stack_trace: stack_trace
      - labels:
          level:
          pipeline:
          dag_id:
          task:
          api_provider:
          signal_type:
      - timestamp:
          source: timestamp
          format: RFC3339Nano
          fallback_formats:
            - UnixMs
      - output:
          source: message

  # Pipeline execution logs
  - job_name: pipeline_execution
    static_configs:
      - targets:
          - localhost
        labels:
          job: pipeline_execution
          __path__: /var/log/investment-signals/executions/**/*.json
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            pipeline: pipeline
            dag_id: dag_id
            run_id: run_id
            task_id: task_id
            status: status
            duration_seconds: duration_seconds
            records_processed: records_processed
            signals_detected: signals_detected
            api_calls: api_calls
            api_cost: api_cost
            error: error
      - labels:
          level:
          pipeline:
          dag_id:
          task_id:
          status:
      - timestamp:
          source: timestamp
          format: RFC3339Nano
      - metrics:
          pipeline_duration:
            type: Histogram
            description: Pipeline execution duration
            source: duration_seconds
            config:
              buckets: [60, 300, 600, 1800, 3600, 7200]
          records_processed:
            type: Counter
            description: Records processed
            source: records_processed
          signals_detected:
            type: Counter
            description: Signals detected
            source: signals_detected

  # Error logs (dedicated error tracking)
  - job_name: errors
    static_configs:
      - targets:
          - localhost
        labels:
          job: errors
          __path__: /var/log/investment-signals/**/error*.log
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            error_type: error_type
            error_code: error_code
            pipeline: pipeline
            dag_id: dag_id
            task_id: task_id
            stack_trace: stack_trace
      - labels:
          level:
          error_type:
          error_code:
          pipeline:
          dag_id:
          task_id:
      - timestamp:
          source: timestamp
          format: RFC3339Nano
      - match:
          selector: '{level="ERROR"} |= "Exception"'
          stages:
            - metrics:
                exceptions_total:
                  type: Counter
                  description: Total exceptions
                  source: error_type

# Target synchronization configuration
target_config:
  sync_period: 10s
